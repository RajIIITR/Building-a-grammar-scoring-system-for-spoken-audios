{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":97919,"databundleVersionId":11694977,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:38:01.826251Z","iopub.execute_input":"2025-04-07T05:38:01.826650Z","iopub.status.idle":"2025-04-07T05:38:02.325263Z","shell.execute_reply.started":"2025-04-07T05:38:01.826607Z","shell.execute_reply":"2025-04-07T05:38:02.324621Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Doing translation of a Sample audio .wev file to text**","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nimport torch\nimport torchaudio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:38:02.326199Z","iopub.execute_input":"2025-04-07T05:38:02.326437Z","iopub.status.idle":"2025-04-07T05:38:25.045177Z","shell.execute_reply.started":"2025-04-07T05:38:02.326418Z","shell.execute_reply":"2025-04-07T05:38:25.044487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:38:25.046637Z","iopub.execute_input":"2025-04-07T05:38:25.047183Z","iopub.status.idle":"2025-04-07T05:38:25.125002Z","shell.execute_reply.started":"2025-04-07T05:38:25.047159Z","shell.execute_reply":"2025-04-07T05:38:25.124092Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Loading basic things required for whisper translation <br>\nReference: HuggingFace, Medium","metadata":{}},{"cell_type":"code","source":"# Model identifier\nmodel_id = \"openai/whisper-large-v3-turbo\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:38:25.126406Z","iopub.execute_input":"2025-04-07T05:38:25.126646Z","iopub.status.idle":"2025-04-07T05:38:25.144065Z","shell.execute_reply.started":"2025-04-07T05:38:25.126625Z","shell.execute_reply":"2025-04-07T05:38:25.143263Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the model and processor\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True)\nmodel.to(device)\nprocessor = AutoProcessor.from_pretrained(model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:38:25.144969Z","iopub.execute_input":"2025-04-07T05:38:25.145214Z","iopub.status.idle":"2025-04-07T05:38:35.946143Z","shell.execute_reply.started":"2025-04-07T05:38:25.145195Z","shell.execute_reply":"2025-04-07T05:38:35.945433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the pipeline\nasr_pipeline = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    torch_dtype=torch_dtype,\n    device=device,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:38:35.946890Z","iopub.execute_input":"2025-04-07T05:38:35.947132Z","iopub.status.idle":"2025-04-07T05:38:35.952861Z","shell.execute_reply.started":"2025-04-07T05:38:35.947110Z","shell.execute_reply":"2025-04-07T05:38:35.951960Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sample Audio file path (Example)\nfile_path = \"/kaggle/input/shl-intern-hiring-assessment/dataset/audios_train/audio_1024.wav\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:38:35.953580Z","iopub.execute_input":"2025-04-07T05:38:35.953770Z","iopub.status.idle":"2025-04-07T05:38:39.654045Z","shell.execute_reply.started":"2025-04-07T05:38:35.953753Z","shell.execute_reply":"2025-04-07T05:38:39.652995Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load audio and preprocess\naudio, sr = librosa.load(file_path, sr=16000, mono=True)  # Ensure mono and 16kHz\ninputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\").to(device, torch_dtype)\n\n# Generate token ids and decode\nwith torch.no_grad():\n    generated_ids = model.generate(inputs[\"input_features\"])\n\n# Decode the predicted tokens into text\ntranscription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\nprint(\"Transcription:\", transcription)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:39:00.750806Z","iopub.execute_input":"2025-04-07T05:39:00.751240Z","iopub.status.idle":"2025-04-07T05:39:15.259493Z","shell.execute_reply.started":"2025-04-07T05:39:00.751196Z","shell.execute_reply":"2025-04-07T05:39:15.258721Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The whisper model can handle input audio in one shot is usually 30 seconds max per segment. (Approx)","metadata":{}},{"cell_type":"markdown","source":"So we will perform silding window technique so it captures first 30 sec then followed by remaining time so that in total it captures the entire 60 seconds audio and give us text translation","metadata":{}},{"cell_type":"code","source":"def transcribe_audio(file_path, segment_length=30):\n    # Load audio\n    waveform, sr = torchaudio.load(file_path)\n\n    # Convert to mono if stereo\n    if waveform.size(0) > 1:\n        waveform = waveform.mean(dim=0, keepdim=True)\n\n    # Resample to 16kHz if not already\n    if sr != 16000:\n        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n        waveform = resampler(waveform)\n\n    # Number of samples in each segment\n    segment_samples = segment_length * 16000  # 30 sec chunks\n\n    transcriptions = []\n\n    for start in range(0, waveform.size(1), segment_samples):\n        end = min(start + segment_samples, waveform.size(1))\n        segment = waveform[:, start:end].squeeze().numpy()\n\n        try:\n            result = asr_pipeline(segment)\n            transcriptions.append(result['text'])\n        except Exception as e:\n            print(f\"Error transcribing segment {start}-{end}: {e}\")\n            transcriptions.append(\"\")\n\n    return \" \".join(transcriptions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:39:15.260362Z","iopub.execute_input":"2025-04-07T05:39:15.260885Z","iopub.status.idle":"2025-04-07T05:39:15.266786Z","shell.execute_reply.started":"2025-04-07T05:39:15.260862Z","shell.execute_reply":"2025-04-07T05:39:15.265835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text = transcribe_audio(file_path)\nprint(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:39:15.268332Z","iopub.execute_input":"2025-04-07T05:39:15.268645Z","iopub.status.idle":"2025-04-07T05:39:16.830036Z","shell.execute_reply.started":"2025-04-07T05:39:15.268618Z","shell.execute_reply":"2025-04-07T05:39:16.829305Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Now we had done transcription with respect to few example.\nLets do transcription of entire audios_train to their respective translation in text format and map it to their respective audio file in train_csv**","metadata":{}},{"cell_type":"markdown","source":"Loading all library","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nimport numpy as np\nimport librosa\nimport time\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:38:57.232103Z","iopub.execute_input":"2025-04-07T05:38:57.232421Z","iopub.status.idle":"2025-04-07T05:38:57.242237Z","shell.execute_reply.started":"2025-04-07T05:38:57.232394Z","shell.execute_reply":"2025-04-07T05:38:57.241320Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_whisper_model():\n    \"\"\"\n    Load and initialize the Whisper model for speech-to-text transcription.\n    \n    Returns:\n        pipeline: Hugging Face pipeline for automatic speech recognition\n\n    Reference:\n        HuggingFace \n    \"\"\"\n    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n    print(f\"Using device: {device}\")\n    \n    model_id = \"openai/whisper-large-v3-turbo\"\n    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n        model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n    )\n    model.to(device)\n    \n    processor = AutoProcessor.from_pretrained(model_id)\n    pipe = pipeline(\n        \"automatic-speech-recognition\",\n        model=model,\n        tokenizer=processor.tokenizer,\n        feature_extractor=processor.feature_extractor,\n        torch_dtype=torch_dtype,\n        device=device,\n    )\n    \n    return pipe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:39:16.831405Z","iopub.execute_input":"2025-04-07T05:39:16.831691Z","iopub.status.idle":"2025-04-07T05:39:16.836947Z","shell.execute_reply.started":"2025-04-07T05:39:16.831667Z","shell.execute_reply":"2025-04-07T05:39:16.836188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_audio(file_path):\n    \"\"\"\n    Load an audio file and convert it to mono if stereo.\n    \n    Args:\n        file_path: Path to the audio file\n        \n    Returns:\n        audio_data: Audio data as numpy array\n        sample_rate: Sample rate of the audio\n     Sample rate refers to the number of audio samples taken per second when digitizing an audio signal. (kHz unit)\n    \"\"\"\n    audio, sample_rate = librosa.load(file_path, sr=16000, mono=False)\n    \n    # Check if audio is stereo (2D array) and convert to mono if needed\n    if len(audio.shape) > 1 and audio.shape[0] == 2:\n        print(f\"Converting stereo to mono for {os.path.basename(file_path)}\")\n        audio = np.mean(audio, axis=0)\n    \n    return audio, sample_rate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:39:16.837723Z","iopub.execute_input":"2025-04-07T05:39:16.837908Z","iopub.status.idle":"2025-04-07T05:39:16.850103Z","shell.execute_reply.started":"2025-04-07T05:39:16.837892Z","shell.execute_reply":"2025-04-07T05:39:16.849371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def transcribe_long_audio(pipe, audio, sample_rate, window_size=30, overlap=1):\n    \"\"\"\n    Transcribe audio that may be longer than 30 seconds using a sliding window approach.\n    \n    Args:\n        pipe: Whisper pipeline for transcription (Obtain from load_whisper_model function)\n        audio: Audio data as numpy array  (Obtain from load_audio)\n        sample_rate: Sample rate of the audio  (Obtain from load_audio)\n        window_size: Size of the sliding window in seconds   (Since whisper model can translate 30 sec audio at a time we had used sliding window technique in our work)\n        overlap: Overlap between windows in seconds   (Overlapping the time frame between two windows)\n        \n    Returns:\n        full_transcription: Complete transcription of the audio\n    \"\"\"\n    # Calculate window and stride sizes in samples\n    window_samples = window_size * sample_rate\n    stride_samples = (window_size - overlap) * sample_rate\n    \n    # If audio is shorter than window_size, just transcribe it directly\n    if len(audio) <= window_samples:\n        result = pipe({\"sampling_rate\": sample_rate, \"raw\": audio})\n        return result[\"text\"].strip()\n    \n    # For longer audio, use sliding window approach\n    transcriptions = []\n    \n    # Calculate number of windows\n    num_windows = max(1, int(np.ceil((len(audio) - window_samples) / stride_samples)) + 1)\n    \n    for i in range(num_windows):\n        start_sample = int(i * stride_samples)\n        end_sample = min(len(audio), start_sample + window_samples)\n        \n        # Extract audio segment\n        audio_segment = audio[start_sample:end_sample]\n        \n        # Transcribe segment\n        result = pipe({\"sampling_rate\": sample_rate, \"raw\": audio_segment})\n        transcriptions.append(result[\"text\"].strip())\n    \n    # Join all transcriptions\n    full_transcription = \" \".join(transcriptions)\n    return full_transcription","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:39:16.850954Z","iopub.execute_input":"2025-04-07T05:39:16.851238Z","iopub.status.idle":"2025-04-07T05:39:16.864542Z","shell.execute_reply.started":"2025-04-07T05:39:16.851210Z","shell.execute_reply":"2025-04-07T05:39:16.863728Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Now we will map all audio translates with their Audio file name in train_csv file**","metadata":{}},{"cell_type":"code","source":"def transcribe_audio_files(audio_folder, csv_path):\n    \"\"\"\n    Transcribe all audio files listed in the CSV and add transcriptions as a new column.\n    \n    Args:\n        audio_folder: Path to folder containing audio files\n        csv_path: Path to CSV file with list of audio files\n        \n    Returns:\n        df: DataFrame with added transcription column\n    \"\"\"\n\n    # Load CSV file\n    df = pd.read_csv(csv_path)\n    print(f\"Loaded CSV with {len(df)} entries\")\n    \n    # Load Whisper model\n    pipe = load_whisper_model()\n    \n    # Create a new column for transcriptions\n    df['transcription'] = \"\"\n    \n    # Loop through each audio file and transcribe\n    print(\"Starting transcription process...\")\n    for i, row in df.iterrows():\n        audio_file = row['filename']  # Adjust column name if needed\n        full_path = os.path.join(audio_folder, audio_file)\n        \n        try:\n            # Print progress\n            print(f\"Processing file {i+1}/{len(df)}: {audio_file}\")\n            \n            # Load and convert audio file\n            audio, sample_rate = load_audio(full_path)\n            \n            # Transcribe audio\n            transcription = transcribe_long_audio(pipe, audio, sample_rate)\n            \n            # Add transcription to dataframe\n            df.at[i, 'transcription'] = transcription\n            \n            # Add a small delay to prevent overloading\n            if (i + 1) % 10 == 0:\n                print(f\"Processed {i+1} files. Taking a short break...\")\n                time.sleep(1)\n                \n        except Exception as e:\n            print(f\"Error processing {audio_file}: {str(e)}\")\n            df.at[i, 'transcription'] = \"ERROR: Could not transcribe\"\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:39:16.865207Z","iopub.execute_input":"2025-04-07T05:39:16.865442Z","iopub.status.idle":"2025-04-07T05:39:16.875929Z","shell.execute_reply.started":"2025-04-07T05:39:16.865424Z","shell.execute_reply":"2025-04-07T05:39:16.875057Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    \"\"\"\n    Main function to run the transcription process.\n    \"\"\"\n    # Define paths\n    audio_folder = \"/kaggle/input/shl-intern-hiring-assessment/dataset/audios_train\"\n    csv_path = \"/kaggle/input/shl-intern-hiring-assessment/dataset/train.csv\"\n    output_path = \"/kaggle/working/train_with_transcriptions.csv\"\n    \n    # Transcribe audio files\n    df = transcribe_audio_files(audio_folder, csv_path)\n    \n    # Save the updated dataframe\n    df.to_csv(output_path, index=False)\n    print(f\"Transcription complete! Saved to {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:39:16.877805Z","iopub.execute_input":"2025-04-07T05:39:16.878006Z","iopub.status.idle":"2025-04-07T05:39:16.891144Z","shell.execute_reply.started":"2025-04-07T05:39:16.877988Z","shell.execute_reply":"2025-04-07T05:39:16.890267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:39:16.892100Z","iopub.execute_input":"2025-04-07T05:39:16.892397Z","iopub.status.idle":"2025-04-07T06:24:49.299546Z","shell.execute_reply.started":"2025-04-07T05:39:16.892370Z","shell.execute_reply":"2025-04-07T06:24:49.298769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Print some examples\n# print(\"\\nExample transcriptions:\")\n# for i in range(min(5, len(df))):\n#     print(f\"File: {df.iloc[i]['file_name']}\")\n#     print(f\"Transcription: {df.iloc[i]['transcription']}\")\n#     print(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:38:40.828762Z","iopub.status.idle":"2025-04-07T05:38:40.829000Z","shell.execute_reply":"2025-04-07T05:38:40.828904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n# Display the first 5 rows of the dataframe with transcriptions\ndf = pd.read_csv(\"/kaggle/working/train_with_transcriptions.csv\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T06:24:49.300946Z","iopub.execute_input":"2025-04-07T06:24:49.301222Z","iopub.status.idle":"2025-04-07T06:24:49.325220Z","shell.execute_reply.started":"2025-04-07T06:24:49.301193Z","shell.execute_reply":"2025-04-07T06:24:49.324570Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the DataFrame to the working directory\noutput_file_path = \"/kaggle/working/transcribed_audio_data.csv\"\ndf.to_csv(output_file_path, index=False)\nprint(f\"Saved transcribed data to {output_file_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T06:24:49.326461Z","iopub.execute_input":"2025-04-07T06:24:49.326762Z","iopub.status.idle":"2025-04-07T06:24:49.338639Z","shell.execute_reply.started":"2025-04-07T06:24:49.326732Z","shell.execute_reply":"2025-04-07T06:24:49.337993Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/shl-intern-hiring-assessment/dataset/train.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T06:24:49.339474Z","iopub.execute_input":"2025-04-07T06:24:49.339741Z","iopub.status.idle":"2025-04-07T06:24:49.348106Z","shell.execute_reply.started":"2025-04-07T06:24:49.339712Z","shell.execute_reply":"2025-04-07T06:24:49.347411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train['label'].min()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T06:24:49.350176Z","iopub.execute_input":"2025-04-07T06:24:49.350433Z","iopub.status.idle":"2025-04-07T06:24:49.355567Z","shell.execute_reply.started":"2025-04-07T06:24:49.350412Z","shell.execute_reply":"2025-04-07T06:24:49.354667Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train['label'].max()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T06:24:49.356667Z","iopub.execute_input":"2025-04-07T06:24:49.356946Z","iopub.status.idle":"2025-04-07T06:24:49.368615Z","shell.execute_reply.started":"2025-04-07T06:24:49.356919Z","shell.execute_reply":"2025-04-07T06:24:49.367713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['transcription'].isnull().any()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T06:24:49.369459Z","iopub.execute_input":"2025-04-07T06:24:49.369750Z","iopub.status.idle":"2025-04-07T06:24:49.380411Z","shell.execute_reply.started":"2025-04-07T06:24:49.369724Z","shell.execute_reply":"2025-04-07T06:24:49.379572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train['label'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T06:24:49.381394Z","iopub.execute_input":"2025-04-07T06:24:49.381662Z","iopub.status.idle":"2025-04-07T06:24:49.402547Z","shell.execute_reply.started":"2025-04-07T06:24:49.381635Z","shell.execute_reply":"2025-04-07T06:24:49.401708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import TrainingArguments, Trainer\nfrom datasets import Dataset\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom sklearn.metrics import mean_squared_error\n\n# Load the transcribed CSV file\ndf = pd.read_csv(\"/kaggle/working/transcribed_audio_data.csv\")\n\n# Drop the filename column\ndf_new = df.drop('filename', axis=1)\n\n# Display the dataframe structure\nprint(\"DataFrame structure after dropping filename column:\")\nprint(df_new.head())\n\n# Convert to dataset format for Hugging Face\ndf_new['label'] = df_new['label'].astype(float)  # Ensure labels are floats\n\n# Use entire dataset for training\ntrain_dataset = Dataset.from_pandas(df_new)\n\n# Load tokenizer and model\nmodel_name = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Define tokenization function\ndef tokenize_function(examples):\n    return tokenizer(examples[\"transcription\"], padding=\"max_length\", truncation=True, max_length=512)\n\n# Tokenize dataset\ntokenized_train = train_dataset.map(tokenize_function, batched=True)\n\n# Initialize DistilBERT model for regression\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=1,  # Single output for regression\n    problem_type=\"regression\"\n)\n\n# Configure QLoRA\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    r=16,  # Rank\n    lora_alpha=32,\n    lora_dropout=0.1,\n    bias=\"none\",\n    target_modules=[\"q_lin\", \"v_lin\", \"k_lin\", \"out_lin\"]  # DistilBERT attention layers\n)\n\n# Apply QLoRA to model\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()  # Show trainable vs total parameters\n\n# Fixed custom trainer class with MSE loss\nclass RegressionTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        labels = inputs.get(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss_fct = nn.MSELoss()\n        loss = loss_fct(logits.view(-1), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n\n# Training arguments - utilizing both T4 GPUs\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/distilbert-audio-regression\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    num_train_epochs=100,\n    weight_decay=0.01,\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    fp16=True,\n    report_to=\"none\",\n    dataloader_num_workers=2,\n    # Add explicit logging settings\n    logging_dir=\"/kaggle/working/logs\",\n    logging_strategy=\"steps\",\n    logging_steps=10,  # Log every 10 steps\n    logging_first_step=True,  # Log the first step\n)\n\n# Initialize custom trainer with MSE loss\ntrainer = RegressionTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    tokenizer=tokenizer,\n)\n\n# Train model\ntrainer.train()\n\n# Save the model\ntrainer.save_model(\"/kaggle/working/distilbert-audio-regression-final\")\nprint(\"Model training complete and model saved!\")\n\n# Create a simple function to make predictions with the trained model\ndef predict_rating(text, model, tokenizer):\n    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    with torch.no_grad():\n        outputs = model(**inputs)\n    return outputs.logits.cpu().numpy().squeeze()\n\n# Example prediction function (can be used after training)\nprint(\"Prediction function created. You can use predict_rating(text, model, tokenizer) to make predictions.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T08:04:31.422969Z","iopub.execute_input":"2025-04-07T08:04:31.423364Z","iopub.status.idle":"2025-04-07T08:21:47.955622Z","shell.execute_reply.started":"2025-04-07T08:04:31.423317Z","shell.execute_reply":"2025-04-07T08:21:47.954509Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport random\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom peft import PeftModel, PeftConfig\n\n# Load the dataset\ndf = pd.read_csv(\"/kaggle/working/train_with_transcriptions.csv\")\n\n# Load the saved model and tokenizer\nmodel_path = \"/kaggle/working/distilbert-audio-regression-final\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# Load the base model and then apply the PEFT adapter\nbase_model_name = \"distilbert-base-uncased\"\nbase_model = AutoModelForSequenceClassification.from_pretrained(\n    base_model_name,\n    num_labels=1,\n    problem_type=\"regression\"\n)\nmodel = PeftModel.from_pretrained(base_model, model_path)\nmodel.eval()\n\n# Define prediction function\ndef predict_rating(text, model, tokenizer):\n    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    with torch.no_grad():\n        outputs = model(**inputs)\n    return outputs.logits.cpu().numpy().squeeze()\n\n# Select random samples and make predictions\nrandom.seed(42)  # For reproducibility\nnum_samples = 5\nsample_indices = random.sample(range(len(df)), num_samples)\n\nprint(f\"\\nTesting model on {num_samples} random samples from training data:\\n\")\nprint(\"-\" * 80)\n\nfor idx in sample_indices:\n    sample = df.iloc[idx]\n    transcription = sample['transcription']\n    true_label = sample['label']\n    \n    # Get prediction\n    predicted_label = predict_rating(transcription, model, tokenizer)\n    \n    # Calculate error\n    error = abs(predicted_label - true_label)\n    \n    # Print results\n    print(f\"Sample {idx+1}:\")\n    print(f\"Transcription (truncated): {transcription[:100]}...\")\n    print(f\"True label: {true_label:.2f}\")\n    print(f\"Predicted label: {predicted_label:.2f}\")\n    print(f\"Absolute error: {error:.2f}\")\n    print(\"-\" * 80)\n\n# Calculate MSE for the random samples\nmse_samples = []\nfor idx in sample_indices:\n    sample = df.iloc[idx]\n    transcription = sample['transcription']\n    true_label = sample['label']\n    predicted_label = predict_rating(transcription, model, tokenizer)\n    mse_samples.append((predicted_label - true_label) ** 2)\n\nmse = sum(mse_samples) / len(mse_samples)\nprint(f\"Mean Squared Error (MSE) for these samples: {mse:.4f}\")\nprint(f\"Root Mean Squared Error (RMSE): {mse ** 0.5:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T08:21:47.957140Z","iopub.execute_input":"2025-04-07T08:21:47.957517Z","iopub.status.idle":"2025-04-07T08:21:50.828072Z","shell.execute_reply.started":"2025-04-07T08:21:47.957487Z","shell.execute_reply":"2025-04-07T08:21:50.827210Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Now creating code for submission**","metadata":{}},{"cell_type":"code","source":"def main():\n    \"\"\"\n    Main function to run the transcription process.\n    \"\"\"\n    # Define paths\n    audio_folder = \"/kaggle/input/shl-intern-hiring-assessment/dataset/audios_test\"\n    csv_path = \"/kaggle/input/shl-intern-hiring-assessment/dataset/test.csv\"\n    output_path = \"/kaggle/working/test_with_transcriptions.csv\"\n    \n    # Transcribe audio files\n    df = transcribe_audio_files(audio_folder, csv_path)\n    \n    # Save the updated dataframe\n    df.to_csv(output_path, index=False)\n    print(f\"Transcription complete! Saved to {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:03:08.787946Z","iopub.execute_input":"2025-04-07T09:03:08.788284Z","iopub.status.idle":"2025-04-07T09:03:08.793383Z","shell.execute_reply.started":"2025-04-07T09:03:08.788257Z","shell.execute_reply":"2025-04-07T09:03:08.792211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:03:10.232839Z","iopub.execute_input":"2025-04-07T09:03:10.233123Z","iopub.status.idle":"2025-04-07T09:21:45.568750Z","shell.execute_reply.started":"2025-04-07T09:03:10.233098Z","shell.execute_reply":"2025-04-07T09:21:45.567820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n# Display the first 5 rows of the dataframe with transcriptions\ndf_test = pd.read_csv(\"/kaggle/working/test_with_transcriptions.csv\")\ndf_test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:22:26.436709Z","iopub.execute_input":"2025-04-07T09:22:26.437026Z","iopub.status.idle":"2025-04-07T09:22:26.449277Z","shell.execute_reply.started":"2025-04-07T09:22:26.436995Z","shell.execute_reply":"2025-04-07T09:22:26.448388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Test_df = df_test.drop('filename', axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:28:58.179725Z","iopub.execute_input":"2025-04-07T09:28:58.180004Z","iopub.status.idle":"2025-04-07T09:28:58.184325Z","shell.execute_reply.started":"2025-04-07T09:28:58.179982Z","shell.execute_reply":"2025-04-07T09:28:58.183511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Test_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:29:02.105994Z","iopub.execute_input":"2025-04-07T09:29:02.106307Z","iopub.status.idle":"2025-04-07T09:29:02.113542Z","shell.execute_reply.started":"2025-04-07T09:29:02.106277Z","shell.execute_reply":"2025-04-07T09:29:02.112742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load our fine-tuned model and tokenizer\nprint(\"Loading fine-tuned model...\")\nmodel_path = \"/kaggle/working/distilbert-audio-regression-final\"\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_path,\n    num_labels=1,\n    problem_type=\"regression\"\n)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nmodel.eval()\n\n# Function to predict scores\ndef predict_rating(text, model, tokenizer, device):\n    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    # Get the prediction\n    prediction = outputs.logits.cpu().numpy().squeeze()\n    return prediction\n\n# Make predictions on test data\nprint(\"Making predictions...\")\npredictions = []\n\nfor text in df_test['transcription']:\n    score = predict_rating(text, model, tokenizer, device)\n    predictions.append(score)\n\n# Add predictions to the test dataframe\ndf_test['label'] = predictions\n\n# Create final submission DataFrame with only required columns\nsubmission_df = pd.DataFrame({\n    'filename': df_test['filename'],\n    'label': df_test['label']\n})\n\n# Save the submission file\nsubmission_df.to_csv(\"/kaggle/working/submission.csv\", index=False)\nprint(\"Predictions completed\")\nprint(f\"Submission file created with {len(submission_df)} entries\")\nprint(\"First few predictions:\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:32:05.758009Z","iopub.execute_input":"2025-04-07T09:32:05.758300Z","iopub.status.idle":"2025-04-07T09:32:10.542067Z","shell.execute_reply.started":"2025-04-07T09:32:05.758277Z","shell.execute_reply":"2025-04-07T09:32:10.541420Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_submission = pd.read_csv(\"/kaggle/working/submission.csv\")\ndf_submission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:39:53.612431Z","iopub.execute_input":"2025-04-07T09:39:53.612722Z","iopub.status.idle":"2025-04-07T09:39:53.623309Z","shell.execute_reply.started":"2025-04-07T09:39:53.612697Z","shell.execute_reply":"2025-04-07T09:39:53.622241Z"}},"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"         filename     label\n0   audio_706.wav  3.725005\n1   audio_800.wav  2.367599\n2    audio_68.wav  3.089059\n3  audio_1267.wav  2.968229\n4   audio_683.wav  2.736108","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>audio_706.wav</td>\n      <td>3.725005</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>audio_800.wav</td>\n      <td>2.367599</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>audio_68.wav</td>\n      <td>3.089059</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>audio_1267.wav</td>\n      <td>2.968229</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>audio_683.wav</td>\n      <td>2.736108</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}